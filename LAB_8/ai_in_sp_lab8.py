# -*- coding: utf-8 -*-
"""AI_IN_SP_LAB8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kZEfw6vqUQEALhrCE7INWXwSacOn6nIh
"""

# A1

import numpy as np
import librosa
import matplotlib.pyplot as plt

# Function to extract features (STFT, MFCC, LPC coefficients) from audio files
def extract_features(audio_file, feature='mfcc', n_mfcc=13):
    y, sr = librosa.load(audio_file)
    if feature == 'stft':
        feature = np.abs(librosa.stft(y))
    elif feature == 'mfcc':
        feature = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    elif feature == 'lpc':
        feature = librosa.lpc(y, order=n_mfcc)
    return feature, sr

# Path to the audio file
audio_path = "/content/AI_IN_SP_AUDIO_RECORDING.wav"

# Load data and extract features
audio_features, sr = extract_features(audio_path)

# Plot the signal
plt.figure(figsize=(10, 4))
librosa.display.waveshow(audio_features, sr=sr)
plt.title('Audio Signal')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.show()

# A1
import numpy as np
import librosa
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense

# Function to extract features (STFT, MFCC, LPC coefficients) from audio files
def extract_features(audio_file, feature='mfcc', n_mfcc=13):
    y, sr = librosa.load(audio_file)
    if feature == 'stft':
        feature = np.abs(librosa.stft(y))
    elif feature == 'mfcc':
        feature = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    elif feature == 'lpc':
        feature = librosa.lpc(y, order=n_mfcc)
    return feature

# Load data
bhanumathi_weds_rajat_features = extract_features('Bhanumathi_weds_Rajat.wav')
bharat_features = extract_features('Bharat.wav')

# Build LSTM model
model = Sequential()
model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=bhanumathi_weds_rajat_features.shape))
model.add(Bidirectional(LSTM(32)))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train model (assuming labels are available)
# model.fit(X_train, y_train, epochs=10, batch_size=32)

# Evaluate model
# loss, accuracy = model.evaluate(X_test, y_test)

# Use the model for speech recognition task
# predicted_labels = model.predict(test_data)

import numpy as np
import librosa
import matplotlib.pyplot as plt

# Function to extract features (STFT, MFCC, LPC coefficients) from audio files
def extract_features(audio_file, feature='mfcc', n_mfcc=13):
    y, sr = librosa.load(audio_file)
    if feature == 'stft':
        feature = np.abs(librosa.stft(y))
    elif feature == 'mfcc':
        feature = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    elif feature == 'lpc':
        feature = librosa.lpc(y, order=n_mfcc)
    return feature, sr

# Load data and extract features
bhanumathi_weds_rajat_features, sr1 = extract_features('Bhanumathi_weds_Rajat.wav')
bharat_features, sr2 = extract_features('Bharat.wav')

# Plot the signals
plt.figure(figsize=(14, 5))

# Plot Bhanumathi weds Rajat
plt.subplot(1, 2, 1)
librosa.display.waveshow(bhanumathi_weds_rajat_features, sr=sr1)
plt.title('Bhanumathi weds Rajat')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')

# Plot Bharat
plt.subplot(1, 2, 2)
librosa.display.waveshow(bharat_features, sr=sr2)
plt.title('Bharat')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')

plt.tight_layout()
plt.show()

# A2

import numpy as np
import soundfile as sf
import IPython.display as ipd
import matplotlib.pyplot as plt

# Define phonemes for "Bharat" along with their durations
phonemes_bharat = ['B', 'AA', 'R', 'AH', 'T']
phoneme_durations_bharat = [0.1, 0.2, 0.15, 0.1, 0.15]  # Example durations

# Combine phonemes from "Bhanumathi weds Rajat" to synthesize "Bharat"
synthesized_audio = np.array([])
for phoneme, duration in zip(phonemes_bharat, phoneme_durations_bharat):
    sr = 22050  # Sample rate
    audio_segment = np.random.randn(int(sr * duration)) * 0.5  # Generate white noise

    # Append the audio segment to the synthesized audio
    synthesized_audio = np.append(synthesized_audio, audio_segment)

# Save synthesized audio to a file using soundfile
sf.write('synthesized_bharat.wav', synthesized_audio, sr)

# Plot the signal
plt.figure(figsize=(10, 4))
plt.plot(np.arange(len(synthesized_audio)) / sr, synthesized_audio)
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('Synthesized Audio Signal')
plt.grid(True)
plt.show()

# Listen to the synthesized audio
ipd.Audio('synthesized_bharat.wav')

import numpy as np
import soundfile as sf
import IPython.display as ipd
import matplotlib.pyplot as plt

# Load Bharat.wav
bharat_audio, sr_bharat = sf.read('Bharat.wav')

# Define phonemes for "Bharat" along with their durations
phonemes_bharat = ['B', 'AA', 'R', 'AH', 'T']
phoneme_durations_bharat = [0.1, 0.2, 0.15, 0.1, 0.15]  # Example durations

# Combine phonemes from "Bhanumathi weds Rajat" to synthesize "Bharat"
synthesized_audio = np.array([])
for phoneme, duration in zip(phonemes_bharat, phoneme_durations_bharat):
    sr = sr_bharat  # Sample rate
    audio_segment = np.random.randn(int(sr * duration)) * 0.5  # Generate white noise

    # Append the audio segment to the synthesized audio
    synthesized_audio = np.append(synthesized_audio, audio_segment)

# Save synthesized audio to a file using soundfile
sf.write('synthesized_bharat.wav', synthesized_audio, sr)

# Plot the signal of Bharat.wav
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(np.arange(len(bharat_audio)) / sr_bharat, bharat_audio)
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('Bharat.wav Signal')

# Plot the signal of synthesized Bharat
plt.subplot(1, 2, 2)
plt.plot(np.arange(len(synthesized_audio)) / sr, synthesized_audio)
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('Synthesized Bharat Signal')

plt.tight_layout()
plt.show()





